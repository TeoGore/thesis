1) vedere performance di progetti simili (e cosa utilizzano)

2) problemi: bilanciare dataset (le metriche quindi sono falsate--> guardare f1 score per tutte le classi)
badqueries:
sqlmap e xsser intercetto le request e le salvo e sono tutte badqueries
sort -u per togliere duplicati

#da terminale
wc -l filename //da il numero di righe del file
(per vedere se mi sto perdendo qualche dat)


3) fare Cross Validation per scegliere il modello:
Train, Validation, Test = (80, 10, 10) or (70, 15, 15)

Il dataset di training viene usato per fare il training dei modelli,
poi si fa il testing di tutti con il dataset di validazione
e si controllano le performance per scegliere il modello (classificatore) migliore (model selection).
Scelto il classificatore (per fare un double check) si usa il dataset di test per trovare le performance.

Posso saltare la validazione solo se so già che modello usare (non faccio model selection) e quindi sono
già sicuro che il modello che voglio usare sia il migliore (caso quasi impossibile)

Lo stesso tipo di modello ma con parametri differenti è un modello diverso, perciò se ho due modelli uguali
ma con parametri differenti, vanno entrambi nella validazione per vedere tra tutti quale dei due funziona meglio


4) Metriche:
accuracy: casi corretti rispetto casi sottoposti (va bene con dataset bilanciati)
con dataset sbilanciati si usano altre metriche (f1_score (per tutte le classi), precision e recall)
o anche auc (= area under the curve)

---------------------------------------
TESTING
from analyzer.nome_analyzer import nome_metodo as analyzer
print(analyzer.sentiment('google.it/?username=matteo'))

----------------------------------------
Spiegazione sommaria del machine learning
supervised: ho dataset con label
unsupervised: non ho label, cerco se ci sono strutture (aree omogenee)
reinforcement learning: non si parla di dati, si ha un agente SW che interagisce con un ambiente e ne cambia lo stato avendo un feedback (es: auto che si guida da sola)

vedendola tabellare
feature = colonna
datapoint = riga

problemi del supervised:
regressione: predirre valore reale
classificazione: predirre classe (variabile categorica)

problemi unsupervised:
clustering: trovare strutture
dimensionality reduction: ho un dataset N-dimensionale (ovvero N feature), e riesco ridurre il dataset, per togliere feature inutile o per visualizzare il dataset (se N>3)

Ci sono tanti algo:
per la regressione: linear regression, regressione non lineare (), random forest, reti neurali
per la classificazione (principale problema):
-lineari
- basati su alberi decisionali
- non lineari

logistic regression
naive bayes (utile quando ci sono i testi)
decision tree e random forest che sono la versione migliorata
reti neurali

gli alfo che sono allo stato dell'arte è dimmostrato empiricamente che funzionano meglio:
deep neural network (DB non strutturati in modo rettangolare: img, audio, video, testo)
basati su random forest come adaptiveBoosting e ExtremeGradientBoosting (migliori per DB rettangolari, variabili categoriche)

c'è un ramo del ML, dato che le DNN funzionano bene con tantissimi dati c'è un ramo dell'ML specializzato sulle reti neurali a piu di due layer (il DL)

DL: ramo del ML che specializzato sulle reti neurali a piu di due layer hidden (ovvero quelli centrali, non input e output (quelli estremi))

Si prende il dataset e si divide in T, CV e Testing
training: allena il modello
CV: scelgie il modello
Testing: da le performace del modello

scelgo tra modelli e per stessi modelli ne uso piu di uno cambiando gli iperparametri

scelgo il modello, prendendo quello con performance migliori in CV e poi vedo le performance reali nel TEST (fornisce la metrica reale delle performance del modello)


Dilemma bias-overfitting: VEDERE
overfitting: ho performance molto alta sul dataset di training, ma ho performance basse su CV e testing
byas: ho subito performance basse, gia sul training

Prima si risolve il bias, poi si va avanti con gli errori
Devo costruire un dataset omogeneo!

in questo modo posso fare troubleshooting:
-se ho prestazioni basso in training--> bias (dati che non vanno bene)
-alto su training (80/90) pero basso in CV--> potrei aver dato in CV un dataset molto diverso da quello di training, allora ho fatto bene lo split?
-alto su CV (80/90) pero basso in testing--> potrei aver dato in testing un dataset molto diverso da quello di training, allora ho fatto bene lo split?

per mettere a posto gioco sugli iperparametri

per NLP lo stato dell'arte è: GloveToVec, wordToVec, Embeddings che si sposano bene con le NN

Spiegazioni MIE degli algoritmi usati:

TFIDF:
TF (term frequency) = (#times find a given word in a document) / (#words in that document); è la frequenza del termine
IDF (inverse document frequency) =log_e(#total_documents, #documents where the word is found); è l'importanza del termine
In practice: TF calculates term frequency giving same weights to all words, IDF scales TF value so that rare words weigth more than common
TFIDF is the product (TF*IDF), indicates a weigth that is the importance of the word

SVM:
Creano iperpiano che divide i dati, parametri:
GAMMA indica il peso dei dati, se alto indica che i dati vicino al confine hanno peso alto nell' influenzarne la modifica
gamma alto puo portare a modifiche significative del confine e (nei casi peggiori) overfitting,
mettendo gamma basso anche i dati lontani dal confine hanno peso significativo nella sua modifica per l'ottimizzazione, quindi si ha un confine più morbido
C - deciside per il tradeoff tra: smooth decision boundary (confine di decisione morbido) e classificazione corretta dei punti, se settato male può portare a overfitting
C alto indica che cerco di non fare errori di classificazione (confine complicato, possibile overfitting),
C basso cerca di fare una divisione tra i dati piu morbida, permettendo delle misclassification
I range di valori che viene dato in pratica a C va tra 2^-5 a 2^5 (all'effettivo varia tra 0 e +infinito)
Posso usare le SVM anche per la classificazione con piu classi, basta aggiungere il parametro decision_function_shape nel costruttore, può avere due valori: 'ovo', 'ovr'
OVR - (one vs rest) costruisce N-1 modelli perche ogni modello è un iperpiano che divide una classe da tutto il resto dei dati
OVO - (one vs one) costruisce piu modelli ed è piu lento, valuta per ogni dato tutte le possibili classi, è quindi più robusto all'imbalance
KERNEL TRICK: è un trucco che aggiunge una dimensione ulteriore ai dati per fare in modo che il modello trovato (iperpiano) fitti (separi) meglio i dati
si fa mettendo il parametro kernel nel costruttore, ha valori: 'linear', 'rbf' (radial basis function), '' (polynomial)(sigmoid)

NuSVM:
Si differenziano dalle SVM classiche (C) per il parametro di penalizzazione delle misclassification:
Nu varia tra 0 e 1 (e sarebbe l'equivalente di C), quindi è piu semplice da tarare

PRO:
-si adatta bene a dati N-dimensionali
-molto buono con dataset piccoli

CONS:
-trovare il giusto kernel e i giusti parametri, può essere computazionalmente intenso

KNN:
assegnano classe in base all'esempio di classi piu vicino, si creano degli spazi di Voronoi, ovvero l'area che circonda un punto do esempio (classe fissacta)
in cui non ci sono altri punti piu vicini (ovvero tutti i punti vicino al dato, per i quali la distanza dal dato è minoer della distanza da tutti gli altri dati)
Se si usa la distanza euclidea, gli spazi di Voronoi hanno delle forme poligonali (perche di fatto sono composti dall'intersezione delle linee di confine delle distanze)
Ogni dato è quindi circondato dal suo spazio di Voronoi, se calcolo gli spazi per tutti i dati ho la tassellazione di Voronoi, ovvero una partizione dello spazio con non-overlapping regions
dove ogni regione è dominata da un esempio di training (un dato). Quindi la linea di confine tra gli spazi di classi differenti farà da decisione per assegnare la classe ai nuovi dati.
Il problema è che è un algoritmo con una buona libertà quindi come si adatta bene, soffre anche molto gli outlier ed è soggetto a overfitting

Si può cercare di contrastare il problema facendo in modo che l'algoritmo non cerchi il singolo dato più vicino, me cerchi gli N più vicini e decida in base a quelli.

RandomForest:
Viene generata una "foresta" di decision tree, una volta che viene dato un input da predirre, ogni albero dice la sua predizione
la predizione più comune diventa il valore predetto (come se fosse un classificatore a votazione(majority voting))(nel caso della regressione prende la media degli output)
Di fatto gli alberi sono dei modelli scorrelati, il loro uso simultaneo fa si che il modello finale (combinazione di tutti i modelli scorrelati) abbia una buona precisione (basso bias e varianza)
Il problema è che un cambio di dati (o anche dei dati outlier) possono portare ad alberi molto diversi e quindi a overfitting (dell'albero)
Inoltre il processo di decisione si ferma secondo un criterio, ma piu itera piu è probabile che ha fatto dell'overfitting
Di fatto le decisioni dell'albero vanno a dividere lo spazio dei dati in regioni separate da linee verticali e orizzontali (creando dei rettangoli)

PRO:
-fa sia classificazione che regressione
-si adatta quando ci sono parti grandi del dataset mancanti
-dati i tanti alberi, è robusta all'overfitting
-va bene per dati multidimensionali

CONS:
-non è molto precisa nella regressione (meglio usarla nella classificazione), non riesce a predirre oltre il range del dominio dei dati di training, e overfitta se il dataset di training è particolarmente rumoroso
-c'è poco controllo su quello che il modello fa

PSEUDO-CODICE:
assumo: N dati di training, ne prendo alcuni (in modo randomico) ma con dei rimpiazzi, saranno i dati usati per far crescere un albero
se ci sono M feature (input variable), allora viene preso un numero m tale che per ogni nodo (ogni scelta dell'albero) vengano usate solo m variabili (feature) selezionate a random tra le M
il miglior split sulle m è utilizzato per fare lo split del nodo (decisione nell'albero)
Il valore di m è tenuto costante mentre facciamo crescere la foresta
Viene fatto crescere ogni albero fino alla sua massima estensione (senza pruning)
La predizione viene fatta aggregando i voti degli n alberi

è un classificatore ensemble perchè di fatto assembla dei classificatori che da soli sono deboli ma messi insieme diventano potenti
(come se metto un gruppo di persone ognuna specializzata in una feature diversa, singolarmente non predirranno bene il complesso, ma insieme diventano fenomenali)

altra terminologia utile:
Pruning: di fatto significa potare la pianta, ovvero tagliare dei rami (togliere delle decisioni) all'albero in modo da evitare overfitting
bagging: per fare impovement di stabilità e accuratezza
boosting: meta-algorithm per ridurre bias e varianza in supervised learning




