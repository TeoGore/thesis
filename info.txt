1) vedere performance di progetti simili (e cosa utilizzano)

2) problemi: bilanciare dataset (le metriche quindi sono falsate--> guardare f1 score per tutte le classi)
badqueries:
sqlmap e xsser intercetto le request e le salvo e sono tutte badqueries
sort -u per togliere duplicati

#da terminale
wc -l filename //da il numero di righe del file
(per vedere se mi sto perdendo qualche dat)


3) fare Cross Validation per scegliere il modello:
Train, Validation, Test = (80, 10, 10) or (70, 15, 15)

Il dataset di training viene usato per fare il training dei modelli,
poi si fa il testing di tutti con il dataset di validazione
e si controllano le performance per scegliere il modello (classificatore) migliore (model selection).
Scelto il classificatore (per fare un double check) si usa il dataset di test per trovare le performance.

Posso saltare la validazione solo se so già che modello usare (non faccio model selection) e quindi sono
già sicuro che il modello che voglio usare sia il migliore (caso quasi impossibile)

Lo stesso tipo di modello ma con parametri differenti è un modello diverso, perciò se ho due modelli uguali
ma con parametri differenti, vanno entrambi nella validazione per vedere tra tutti quale dei due funziona meglio


4) Metriche:
accuracy: casi corretti rispetto casi sottoposti (va bene con dataset bilanciati)
con dataset sbilanciati si usano altre metriche (f1_score (per tutte le classi), precision e recall)
o anche auc (= area under the curve)

---------------------------------------
TESTING
from analyzer.nome_analyzer import nome_metodo as analyzer
print(analyzer.sentiment('google.it/?username=matteo'))

----------------------------------------
Spiegazioni MIE degli algoritmi usati:

TFIDF:
TF (term frequency) = (#times find a given word in a document) / (#words in that document); è la frequenza del termine
IDF (inverse document frequency) =log_e(#total_documents, #documents where the word is found); è l'importanza del termine
In practice: TF calculates term frequency giving same weights to all words, IDF scales TF value so that rare words weigth more than common
TFIDF is the product (TF*IDF), indicates a weigth that is the importance of the word

SVM:
Creano iperpiano che divide i dati, parametri:
GAMMA indica il peso dei dati, se alto indica che i dati vicino al confine hanno peso alto nell' influenzarne la modifica
gamma alto puo portare a modifiche significative del confine e (nei casi peggiori) overfitting,
mettendo gamma basso anche i dati lontani dal confine hanno peso significativo nella sua modifica per l'ottimizzazione, quindi si ha un confine più morbido
C - deciside per il tradeoff tra: smooth decision boundary (confine di decisione morbido) e classificazione corretta dei punti, se settato male può portare a overfitting
C alto indica che cerco di non fare errori di classificazione (confine complicato, possibile overfitting),
C basso cerca di fare una divisione tra i dati piu morbida, permettendo delle misclassification
I range di valori che viene dato in pratica a C va tra 2^-5 a 2^5 (all'effettivo varia tra 0 e +infinito)
Posso usare le SVM anche per la classificazione con piu classi, basta aggiungere il parametro decision_function_shape nel costruttore, può avere due valori: 'ovo', 'ovr'
OVR - (one vs rest) costruisce N-1 modelli perche ogni modello è un iperpiano che divide una classe da tutto il resto dei dati
OVO - (one vs one) costruisce piu modelli ed è piu lento, valuta per ogni dato tutte le possibili classi, è quindi più robusto all'imbalance
KERNEL TRICK: è un trucco che aggiunge una dimensione ulteriore ai dati per fare in modo che il modello trovato (iperpiano) fitti (separi) meglio i dati
si fa mettendo il parametro kernel nel costruttore, ha valori: 'linear', 'rbf' (radial basis function), '' (polynomial)(sigmoid)

NuSVM:
Si differenziano dalle SVM classiche (C) per il parametro di penalizzazione delle misclassification:
Nu varia tra 0 e 1 (e sarebbe l'equivalente di C), quindi è piu semplice da tarare

KNN:
assegnano classe in base all'esempio di classi piu vicino, si creano degli spazi di Voronoi, ovvero l'area che circonda un punto do esempio (classe fissacta)
in cui non ci sono altri punti piu vicini (ovvero tutti i punti vicino al dato, per i quali la distanza dal dato è minoer della distanza da tutti gli altri dati)
Se si usa la distanza euclidea, gli spazi di Voronoi hanno delle forme poligonali (perche di fatto sono composti dall'intersezione delle linee di confine delle distanze)
Ogni dato è quindi circondato dal suo spazio di Voronoi, se calcolo gli spazi per tutti i dati ho la tassellazione di Voronoi, ovvero una partizione dello spazio con non-overlapping regions
dove ogni regione è dominata da un esempio di training (un dato). Quindi la linea di confine tra gli spazi di classi differenti farà da decisione per assegnare la classe ai nuovi dati.
Il problema è che è un algoritmo con una buona libertà quindi come si adatta bene, soffre anche molto gli outlier ed è soggetto a overfitting

Si può cercare di contrastare il problema facendo in modo che l'algoritmo non cerchi il singolo dato più vicino, me cerchi gli N più vicini e decida in base a quelli.

RandomForest:


PRO:
-si adatta bene a dati N-dimensionali
-molto buono con dataset piccoli

CONS:
-trovare il giusto kernel e i giusti parametri, può essere computazionalmente intenso



